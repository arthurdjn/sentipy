{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiPy Tutorial\n",
    "\n",
    "### *An Application for Twitter Sentiment Analytics*\n",
    "\n",
    "**SentiPy** provides models to analyze user's sentiments from tweets. The models are based on **Word Embeddings** and **Convolutional Neural Network** (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Implementation\n",
    "\n",
    "## 1.1. Preprocessing the data\n",
    "\n",
    "The dataset used for this models is taken from [Sentiment140](http://www.sentiment140.com/) dataset which is composed of 1.6b tweets. The dataset is availble at [Stanford University](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip). More help can be found at [this link](http://help.sentiment140.com/for-students/).\n",
    "\n",
    "The custom torch dataset *Sentiment140* extract tweets and labels from the csv dataset, and download it in case you don't have it. Because *Sentiment140* class inherits from Dataset class, you have access to splits, iters and others default methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "from sentipy.datasets import Sentiment140\n",
    "from sentipy.tokenizer import tokenizer_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2303cdc8490>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "SEED = 2020\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-ad4630894b26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Get the training data / validation data / test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentiment140\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepneutral\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\home\\Arthur\\Documents\\Informatique\\GitHub\\sentipy\\sentipy\\datasets\\sentiment140.py\u001b[0m in \u001b[0;36msplits\u001b[1;34m(cls, text_field, label_field, root, train, test, neutral, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mpath_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentiment140\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_field\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneutral\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneutral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentiment140\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_field\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\home\\Arthur\\Documents\\Informatique\\GitHub\\sentipy\\sentipy\\datasets\\sentiment140.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, text_field, label_field, keepneutral, neutral, size, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         df = pd.read_csv(path, encoding='latin-1', header=0,\n\u001b[1;32m---> 52\u001b[1;33m                     names=[\"label\", \"id\", \"date\", \"query\", \"user\", \"text\"])\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize = tokenizer_tweets, batch_first = True)\n",
    "LABEL = data.LabelField(preprocessing = lambda y: y//2, dtype = torch.float)\n",
    "\n",
    "# Get the training data / validation data / test data\n",
    "train_data, test_data = Sentiment140.splits(TEXT, LABEL, keepneutral=False, size=50000, shuffle=True)\n",
    "train_data, valid_data = train_data.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels\")\n",
    "print([vars(train_data[i])[\"label\"] for i in range(10)])\n",
    "\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check\n",
      "Training dataset length: 50000\n",
      "\n",
      "Example n°0: {'label': 2, 'id': 1759774113, 'date': 'Sun May 10 19:56:48 PDT 2009', 'query': 'NO_QUERY', 'user': 'melux', 'text': ['playing', 'medal', 'of', 'honor', 'with', 'my', 'brother']}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sanity Check\\nTraining dataset length: {}\".format(len(train_data) + len(valid_data)))\n",
    "print(\"\\nExample n°0: {}\".format(vars(train_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "# Create the vocabulary for words embeddings\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 # vectors = \"glove.twitter.27B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {0: 0, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 21435), ('.', 20468), ('<user>', 19848), ('i', 18989), ('to', 14024), ('the', 13180), (',', 11682), ('a', 9600), ('my', 7758), ('and', 7597), ('...', 6991), ('you', 6784), ('?', 5904), ('is', 5854), ('it', 5826), ('in', 5418), ('for', 5328), ('of', 4635), ('on', 4174), ('me', 4099)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '!', '.', '<user>', 'i', 'to', 'the', ',', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0087, -0.3213, -1.2899,  ...,  0.9333, -0.6701,  0.2908],\n",
      "        [ 1.1236,  0.8868,  1.1304,  ..., -0.9977,  0.0336, -0.3832],\n",
      "        [ 0.3847,  0.4935,  0.4910,  ...,  0.0263,  0.3905,  0.5222],\n",
      "        ...,\n",
      "        [-0.2991, -0.3692,  0.8557,  ...,  0.2474, -0.6483,  0.5548],\n",
      "        [ 0.9752, -1.0016, -1.3873,  ...,  0.6071, -0.0471,  0.3319],\n",
      "        [ 0.3994,  0.9867,  0.4112,  ...,  0.6024, -0.5983,  0.2528]])\n",
      "There are 20002 words in the vocabulary\n",
      "\n",
      "vector: tensor([-1.2655, -0.7780,  0.6882, -0.6532,  0.4089, -0.5350, -1.7817, -0.2428,\n",
      "         0.5133,  1.1793]), size: 100\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = TEXT.vocab.vectors\n",
    "print(glove_vectors)\n",
    "print(f'There are {len(glove_vectors)} words in the vocabulary\\n')\n",
    "\n",
    "idx = TEXT.vocab.itos.index(\"<user>\")\n",
    "print(f\"vector: {glove_vectors[idx][:10]}, size: {len(glove_vectors[idx])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.BucketIterator(train_data, \n",
    "                             shuffle=True,\n",
    "                             batch_size = BATCH_SIZE, \n",
    "                             device = device)\n",
    "valid_iterator = data.BucketIterator(valid_data,\n",
    "                             shuffle=True,\n",
    "                             batch_size = BATCH_SIZE, \n",
    "                             device = device)\n",
    "test_iterator = data.BucketIterator(test_data,\n",
    "                             shuffle=True,\n",
    "                             batch_size = BATCH_SIZE, \n",
    "                             device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 2, 0, 0, 0, 2, 2, 0, 2]\n",
      "tensor([1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 1., 0.])\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print([vars(valid_data[i])[\"label\"] for i in range(10)])\n",
    "for (idx, batch) in enumerate(train_iterator):\n",
    "    print(batch.label)\n",
    "    break\n",
    "for (idx, batch) in enumerate(test_iterator):\n",
    "    print(batch.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sentipy.model import CNN\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 50\n",
    "EMBEDDING_DIM = 100\n",
    "N_FITLERS = 100\n",
    "FILTER_SIZES = [2, 3, 4, 5, 8]\n",
    "OUTPUT_DIM = 2\n",
    "DROPOUT = 0.5\n",
    "ACTIVATION_LAYER = F.relu\n",
    "ACTIVATION_OUTPUT = F.sigmoid\n",
    "LR = 0.1\n",
    "WEIGHT_DECAY = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]   \n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FITLERS, FILTER_SIZES, \n",
    "            OUTPUT_DIM, DROPOUT, pad_idx = PAD_IDX, \n",
    "            activation_layer = ACTIVATION_LAYER,\n",
    "            activation_output = ACTIVATION_OUTPUT)\n",
    "\n",
    "# Optimization\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = LR, weight_decay = WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     :   1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  :   100% | [==================================================] | Time : 3m 19s\n",
      "Validation:   100% | [==================================================] | Time : 0m 26s\n",
      "Stats Training     | Loss: 0.668 | Acc: 59.40% | Prec.: 59.40% | Rec.: 59.41% | F1: 59.40%\n",
      "Stats Validation   | Loss: 0.638 | Acc: 64.84% | Prec.: 64.79% | Rec.: 64.79% | F1: 64.79%\n",
      "\n",
      "Epoch     :   2/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 24s\n",
      "Validation:   100% | [==================================================] | Time : 0m 30s\n",
      "Stats Training     | Loss: 0.636 | Acc: 64.63% | Prec.: 64.63% | Rec.: 64.64% | F1: 64.63%\n",
      "Stats Validation   | Loss: 0.620 | Acc: 67.12% | Prec.: 67.07% | Rec.: 67.22% | F1: 67.07%\n",
      "\n",
      "Epoch     :   3/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 23s\n",
      "Validation:   100% | [==================================================] | Time : 0m 23s\n",
      "Stats Training     | Loss: 0.619 | Acc: 66.89% | Prec.: 66.89% | Rec.: 66.90% | F1: 66.89%\n",
      "Stats Validation   | Loss: 0.610 | Acc: 68.06% | Prec.: 67.99% | Rec.: 68.60% | F1: 67.99%\n",
      "\n",
      "Epoch     :   4/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 26s\n",
      "Validation:   100% | [==================================================] | Time : 0m 28s\n",
      "Stats Training     | Loss: 0.605 | Acc: 68.77% | Prec.: 68.77% | Rec.: 68.78% | F1: 68.77%\n",
      "Stats Validation   | Loss: 0.600 | Acc: 69.47% | Prec.: 69.58% | Rec.: 69.60% | F1: 69.58%\n",
      "\n",
      "Epoch     :   5/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 24s\n",
      "Validation:   100% | [==================================================] | Time : 0m 21s\n",
      "Stats Training     | Loss: 0.596 | Acc: 69.95% | Prec.: 69.95% | Rec.: 69.95% | F1: 69.95%\n",
      "Stats Validation   | Loss: 0.593 | Acc: 70.48% | Prec.: 70.48% | Rec.: 70.50% | F1: 70.48%\n",
      "\n",
      "Epoch     :   6/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 26s\n",
      "Validation:   100% | [==================================================] | Time : 0m 16s\n",
      "Stats Training     | Loss: 0.586 | Acc: 71.41% | Prec.: 71.41% | Rec.: 71.41% | F1: 71.41%\n",
      "Stats Validation   | Loss: 0.589 | Acc: 71.02% | Prec.: 71.16% | Rec.: 71.36% | F1: 71.16%\n",
      "\n",
      "Epoch     :   7/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 24s\n",
      "Validation:   100% | [==================================================] | Time : 0m 23s\n",
      "Stats Training     | Loss: 0.579 | Acc: 72.52% | Prec.: 72.52% | Rec.: 72.52% | F1: 72.52%\n",
      "Stats Validation   | Loss: 0.582 | Acc: 71.88% | Prec.: 71.90% | Rec.: 71.90% | F1: 71.90%\n",
      "\n",
      "Epoch     :   8/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 25s\n",
      "Validation:   100% | [==================================================] | Time : 0m 26s\n",
      "Stats Training     | Loss: 0.570 | Acc: 73.61% | Prec.: 73.61% | Rec.: 73.61% | F1: 73.61%\n",
      "Stats Validation   | Loss: 0.578 | Acc: 72.27% | Prec.: 72.34% | Rec.: 72.36% | F1: 72.34%\n",
      "\n",
      "Epoch     :   9/10\n",
      "Training  :   100% | [==================================================] | Time : 3m 27s\n",
      "Validation:   100% | [==================================================] | Time : 0m 20s\n",
      "Stats Training     | Loss: 0.564 | Acc: 74.36% | Prec.: 74.36% | Rec.: 74.36% | F1: 74.36%\n",
      "Stats Validation   | Loss: 0.575 | Acc: 72.78% | Prec.: 72.80% | Rec.: 72.80% | F1: 72.80%\n",
      "\n",
      "Epoch     :  10/10\n",
      "Training  :   100% | [==================================================] | Time : 4m 20s\n",
      "Validation:   100% | [==================================================] | Time : 0m 20s\n",
      "Stats Training     | Loss: 0.559 | Acc: 75.24% | Prec.: 75.24% | Rec.: 75.24% | F1: 75.24%\n",
      "Stats Validation   | Loss: 0.572 | Acc: 73.14% | Prec.: 73.10% | Rec.: 73.10% | F1: 73.10%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentipy.optim.performer import Performer\n",
    "\n",
    "performer = Performer(model, criterion, optimizer)\n",
    "performer.run(train_iterator, valid_iterator, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test     :   100% | [==================================================] | Time : 0m 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6783168812592825"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer.test(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.5598957190910975,\n",
       " 'accuracy': 0.7533660233020782,\n",
       " 'precision': 0.7493150684931507,\n",
       " 'recall': 0.7527777777777778,\n",
       " 'macro_f1': 0.750273556231003,\n",
       " 'confusion_matrix': [[51, 17], [22, 68]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer.results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test     :   100% | [==================================================] | Time : 0m 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5519537081321081"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer.test(test_iterator, thresholds=(.8, .8), addneutral=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.7613080143928528,\n",
       " 'accuracy': 0.5519537081321081,\n",
       " 'precision': 0.5580808080808081,\n",
       " 'recall': 0.5580528846153846,\n",
       " 'macro_f1': 0.5580332992727054,\n",
       " 'confusion_matrix': [[35, 29], [28, 37]]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer.results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test     :   100% | [==================================================] | Time : 1m 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8859244243621827"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer.test(train_iterator, thresholds=(.8, .8), addneutral=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.42733731942176817,\n",
       " 'accuracy': 0.8859244243621827,\n",
       " 'precision': 0.885114377426824,\n",
       " 'recall': 0.8859048600187107,\n",
       " 'macro_f1': 0.8853957452063368,\n",
       " 'confusion_matrix': [[10125, 1367], [1121, 9141]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer.results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = tokenizer_tweets, batch_first = True)\n",
    "LABEL = data.LabelField(preprocessing = lambda y: y//2, dtype = torch.float)\n",
    "\n",
    "# Get the training data / validation data / test data\n",
    "train_data, test_data = Sentiment140.splits(TEXT, LABEL, keepneutral=False, size=10000, shuffle=True)\n",
    "train_data, valid_data = train_data.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "# Create the vocabulary for words embeddings\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 # vectors = \"glove.twitter.27B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.BucketIterator(train_data, \n",
    "                             shuffle=True,\n",
    "                             batch_size = BATCH_SIZE, \n",
    "                             device = device)\n",
    "valid_iterator = data.BucketIterator(valid_data,\n",
    "                             shuffle=True,\n",
    "                             batch_size = BATCH_SIZE, \n",
    "                             device = device)\n",
    "test_iterator = data.BucketIterator(test_data,\n",
    "                             shuffle=True,\n",
    "                             batch_size = BATCH_SIZE, \n",
    "                             device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test     :   100% | [==================================================] | Time : 0m 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5539246648550034"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performer.test(test_iterator, thresholds=(.8, .8), addneutral=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
